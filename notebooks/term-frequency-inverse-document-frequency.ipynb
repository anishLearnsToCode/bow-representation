{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency Inverse Document Frequency (TF) Vector Representation\n",
    "__Anish Sachdeva (DTU/2K16/MC/013)__\n",
    "\n",
    "__Natural Language Processing (IT-425)__\n",
    "\n",
    "In this noteook we will extract Term Frequency vector representations from a given corpus, where our corpus will be my resume. We will divide the corpus into 6 different parts and each part will be treated as a document. The vector for a given word will be a $1 \\times 6$ vector and each column will represent the frequency countof how many times the word occured in that particular document. \n",
    "\n",
    "## 1. Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the Corpus (Resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anish sachdeva\n",
      "software developer + clean code enthusiast\n",
      "\n",
      "phone : 8287428181\n",
      "email : anish_@outlook.com\n",
      "home : sandesh vihar, pitampura, new delhi - 110034\n",
      "date of birth : 7th april 1998\n",
      "languages : english, hindi, french\n",
      "\n",
      "work experience\n",
      "what after college (4 months)\n",
      "delhi, india\n",
      "creating content to teach core java and python with data structures and algorithms and giving online classes to students.\n",
      "giving python classes workshops to students all around india and teaching core data structures and the python api\n",
      "with emphasis on data structures, algorithms and problem solving. see a sample python batch here:\n",
      "https://github.com/anishlearnstocode/python-workshop-6\n",
      "\n",
      "also teaching java to students in batches of 10 days, where the full java api and data types are covered along with many\n",
      "important algorithms are aso taught. see a sample java batch here: https://github.com/anishlearnstocode/java-wac-batch-32\n",
      "\n",
      "summer research fellow at university of auckland (2 months)\n",
      "auckland, new zealand\n",
      "worked on geometry of mobius transformations, differential geometry under dr. pedram hekmati at the department of\n",
      "mathematics, university of auckland. worked on various topics in mathematics such as abelian group theory,\n",
      "measure theory, graph theory and differential geometry.\n",
      "\n",
      "attended lectures and conferences with notable speakers from throughout academia and the industry. met up with ed witten\n",
      "who is currently at the forefront of applied mathematics in physics and mathematical topology in higher dimensional\n",
      "spaces.\n",
      "\n",
      "met with scientist from microsoft quantum research that are working on cutting edge research and using foundational\n",
      "group theory and mobius transformations in real world practical applications.\n",
      "\n",
      "software developer at cern (14 months)\n",
      "cern, geneva, switzerland\n",
      "worked in the core platforms team of the fap-bc group. part of an agile team of developers that maintains and adds core\n",
      "functionality to applications used internally at cern by hr, financial, administrative and other departments including\n",
      "scientific.\n",
      "\n",
      "worked on legacy applications that comprise of single and some times multiple frameworks such as java spring, boot,\n",
      "hibernate and java ee. also worked with google polymer 1.0 and jsp on the client side.\n",
      "\n",
      "maintained cern's electronic document handing system application with >1m loc that comprising of multiple frameworks\n",
      "and created ~20 years ago. worked on feature requests, support requests and incidents and also release cycles.\n",
      "\n",
      "while at cern, i also engaged socially and participated in self growth outside the work environment. i was part of the\n",
      "department band as lead singer and guitarist. i also worked on my french and learnt it till a2 level. i participated\n",
      "in many workshops, and volunteered as a participant and helper in many activities related to programming, robotics\n",
      "etc.\n",
      "\n",
      "teaching assistant (4 months)\n",
      "coding ninjas, delhi\n",
      "served as the teaching assistant to nucleus - java with ds batch, under mr. ankur kumar. worked on creating course\n",
      "content and quizzes for online platform of coding ninjas for java. helped students in core data structures and algorithms\n",
      "concepts in java.\n",
      "\n",
      "education\n",
      "delhi technological university (2016 - 2021)\n",
      "bachelors of technology mathematics and computing\n",
      "cgpa: 9.2\n",
      "\n",
      "the heritage school rohini (2004 - 2016)\n",
      "physics, chemistry, maths + computer science with english\n",
      "senior secondary: 94.8%\n",
      "secondary: 9.8 cgpa\n",
      "\n",
      "technical skills\n",
      "java + algorithms and data structures\n",
      "mean stack web development\n",
      "python + machine learning\n",
      "matlab + octave\n",
      "mysql, postgressql & mongodb\n",
      "\n",
      "other skills\n",
      "ms office, adobe photoshop, latex + mitex\n",
      "\n",
      "university courses\n",
      "applied mathematics i, ii, iii\n",
      "linear algebra + probability & statistics + stochastic processes + discrete maths\n",
      "computer organization & architecture + data structures + algorithm design and analysis + dbms + os\n",
      "computer vision + nlp\n",
      "\n",
      "important links\n",
      "https://www.linkedin.com/in/anishsachdeva1998/\n",
      "https://github.com/anishlearnstocode\n",
      "https://www.hackerrank.com/anishviewer\n",
      "\n",
      "honours and awards\n",
      "mitacs globalink scholarship cohort of 2020\n",
      "summer research fellowship university of auckland mathematics department\n",
      "technical student @ cern\n",
      "google india challenge scholarship\n",
      "\n",
      "certifications\n",
      "trinity college of london plectrum guitar grade 4 (distinction)\n",
      "trinity college of london plectrum guitar grade 3 (merit)\n",
      "trinity college of london plectrum guitar grade 2 (distinction)\n",
      "trinity college of london plectrum guitar grade 1 (distinction)\n",
      "french a2.1 level from cern\n",
      "java data structures and algorithms @ coding ninjas\n",
      "web development with ruby on rails @ coding ninjas\n",
      "competitive programming @ coding ninjas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_file = open('../assets/resume.txt', 'r')\n",
    "resume = resume_file.read().lower()\n",
    "resume_file.close()\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing The Resume\n",
    "We now create a utility function called `tokenize` that will take in a corpus (resume in this case) and will return us a list of tokens after removing stopwords and punctuations. It will only consider alphabetic words and all numbers have also been ignored.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for tokenizing\n",
    "def tokenize(document: str, stopwords_en=stopwords.words('english'), tokenizer=nltk.RegexpTokenizer(r'\\w+')):\n",
    "    document = document.lower()\n",
    "    return [token for token in tokenizer.tokenize(document) if token not in stopwords_en and token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anish', 'sachdeva', 'software', 'developer', 'clean', 'code', 'enthusiast', 'phone', 'email', 'outlook', 'com', 'home', 'sandesh', 'vihar', 'pitampura', 'new', 'delhi', 'date', 'birth', 'april', 'languages', 'english', 'hindi', 'french', 'work', 'experience', 'college', 'months', 'delhi', 'india']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the resume\n",
    "tokens = tokenize(resume)\n",
    "\n",
    "# see first 30 tokens\n",
    "print(tokens[: 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dividing the Corpus Into 6 Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(tokens) // 6\n",
    "documents = []\n",
    "for i in range(5):\n",
    "    documents.append(tokens[i * k: (i + 1) * k])\n",
    "documents.append(tokens[5 * k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['links',\n",
      " 'https',\n",
      " 'www',\n",
      " 'linkedin',\n",
      " 'com',\n",
      " 'https',\n",
      " 'github',\n",
      " 'com',\n",
      " 'anishlearnstocode',\n",
      " 'https',\n",
      " 'www',\n",
      " 'hackerrank',\n",
      " 'com',\n",
      " 'anishviewer',\n",
      " 'honours',\n",
      " 'awards',\n",
      " 'mitacs',\n",
      " 'globalink',\n",
      " 'scholarship',\n",
      " 'cohort',\n",
      " 'summer',\n",
      " 'research',\n",
      " 'fellowship',\n",
      " 'university',\n",
      " 'auckland',\n",
      " 'mathematics',\n",
      " 'department',\n",
      " 'technical',\n",
      " 'student',\n",
      " 'cern',\n",
      " 'google',\n",
      " 'india',\n",
      " 'challenge',\n",
      " 'scholarship',\n",
      " 'certifications',\n",
      " 'trinity',\n",
      " 'college',\n",
      " 'london',\n",
      " 'plectrum',\n",
      " 'guitar',\n",
      " 'grade',\n",
      " 'distinction',\n",
      " 'trinity',\n",
      " 'college',\n",
      " 'london',\n",
      " 'plectrum',\n",
      " 'guitar',\n",
      " 'grade',\n",
      " 'merit',\n",
      " 'trinity',\n",
      " 'college',\n",
      " 'london',\n",
      " 'plectrum',\n",
      " 'guitar',\n",
      " 'grade',\n",
      " 'distinction',\n",
      " 'trinity',\n",
      " 'college',\n",
      " 'london',\n",
      " 'plectrum',\n",
      " 'guitar',\n",
      " 'grade',\n",
      " 'distinction',\n",
      " 'french',\n",
      " 'level',\n",
      " 'cern',\n",
      " 'java',\n",
      " 'data',\n",
      " 'structures',\n",
      " 'algorithms',\n",
      " 'coding',\n",
      " 'ninjas',\n",
      " 'web',\n",
      " 'development',\n",
      " 'ruby',\n",
      " 'rails',\n",
      " 'coding',\n",
      " 'ninjas',\n",
      " 'competitive',\n",
      " 'programming',\n",
      " 'coding',\n",
      " 'ninjas']\n"
     ]
    }
   ],
   "source": [
    "# the 6th document is \n",
    "pprint.pp(documents[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculating Most Common 5 Tokens From Each Document & Storing Frequency Tables for Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = set()\n",
    "document_frequencies = []\n",
    "for document in documents:\n",
    "    frequencies = Counter(document)\n",
    "    document_frequencies.append(frequencies)\n",
    "    for word, frequency in frequencies.most_common(5):\n",
    "        most_common.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 27\n"
     ]
    }
   ],
   "source": [
    "# number of tokens we have selected, as it isn't necessary to obtain 30 unique tokens\n",
    "print('Number of tokens:', len(most_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from first document: [('python', 5), ('data', 3), ('structures', 3), ('students', 3), ('com', 2)]\n"
     ]
    }
   ],
   "source": [
    "# The tokens from the first document are\n",
    "print('Tokens from first document:', document_frequencies[0].most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithms',\n",
      " 'also',\n",
      " 'applications',\n",
      " 'auckland',\n",
      " 'cern',\n",
      " 'college',\n",
      " 'com',\n",
      " 'computer',\n",
      " 'data',\n",
      " 'geometry',\n",
      " 'group',\n",
      " 'guitar',\n",
      " 'java',\n",
      " 'london',\n",
      " 'many',\n",
      " 'mathematics',\n",
      " 'participated',\n",
      " 'plectrum',\n",
      " 'python',\n",
      " 'requests',\n",
      " 'research',\n",
      " 'structures',\n",
      " 'students',\n",
      " 'theory',\n",
      " 'trinity',\n",
      " 'university',\n",
      " 'worked'}\n"
     ]
    }
   ],
   "source": [
    "# The selected tokens are\n",
    "pprint.pp(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculating Number of Documents a Keyword Appears In\n",
    "The TF-IDF vector for a given word is given by:\n",
    "$$\n",
    "tfidf(w, d) = tf(w, d) \\times idf(w, d) \\\\\n",
    "idf(w, d) = \\log{\\frac{N_t}{N_w}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$N_t:$ is the total numeber of documents and\n",
    "\n",
    "$N_w:$ is the total number of documents containing the keyword $w$.\n",
    "\n",
    "We now create a dictionary `N_w` (_str_ $\\rightarrow$ _int_ ) which will store the number of docuemnts a word $w$ occurrs in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_t = 6\n",
    "N_w = {}\n",
    "for word in most_common:\n",
    "    count = 0\n",
    "    for frequencies in document_frequencies:\n",
    "        count = count + (word in frequencies)\n",
    "    N_w[word] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'requests': 1,\n",
      " 'geometry': 1,\n",
      " 'mathematics': 3,\n",
      " 'university': 3,\n",
      " 'algorithms': 4,\n",
      " 'java': 6,\n",
      " 'college': 2,\n",
      " 'group': 2,\n",
      " 'many': 2,\n",
      " 'com': 3,\n",
      " 'theory': 2,\n",
      " 'python': 2,\n",
      " 'plectrum': 1,\n",
      " 'students': 2,\n",
      " 'london': 1,\n",
      " 'research': 3,\n",
      " 'cern': 3,\n",
      " 'trinity': 1,\n",
      " 'participated': 1,\n",
      " 'guitar': 1,\n",
      " 'data': 5,\n",
      " 'applications': 1,\n",
      " 'worked': 3,\n",
      " 'also': 3,\n",
      " 'structures': 3,\n",
      " 'auckland': 2,\n",
      " 'computer': 1}\n"
     ]
    }
   ],
   "source": [
    "# seeing the N_w map for all the selected words\n",
    "pprint.pp(N_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice above that __java__ is the only word in the given list to appear in all 6 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Computing the TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "for word in most_common:\n",
    "    vector = [0] * 6\n",
    "    for index, frequencies in enumerate(document_frequencies):\n",
    "        vector[index] = frequencies[word] * np.log(N_t / N_w[word])\n",
    "    vectors[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Let's see the vector output for a few words\n",
    "print(vectors['java'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.295836866004329, 0.0, 0.0, 1.0986122886681098, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(vectors['students'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.493061443340549, 0.0, 0.0, 0.0, 1.0986122886681098, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# you can also test it out with a word of your choice, try below:\n",
    "word = 'python'\n",
    "print(vectors.get(word, [0] * 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Representing The Vectors in a Tabular Form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pandas.DataFrame(data=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   requests  geometry  mathematics  university  algorithms  java   college\n",
      "0  0.000000  0.000000     0.000000    0.000000    0.810930   0.0  1.098612\n",
      "1  0.000000  5.375278     2.079442    1.386294    0.405465   0.0  0.000000\n",
      "2  0.000000  0.000000     0.000000    0.000000    0.000000   0.0  0.000000\n",
      "3  3.583519  0.000000     0.000000    0.000000    0.000000   0.0  0.000000\n",
      "4  0.000000  0.000000     1.386294    1.386294    0.810930   0.0  0.000000\n",
      "5  0.000000  0.000000     0.693147    0.693147    0.405465   0.0  4.394449\n"
     ]
    }
   ],
   "source": [
    "print(table.iloc[:, 0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      group      many       com    theory    python  plectrum  students\n",
      "0  0.000000  0.000000  1.386294  0.000000  5.493061  0.000000  3.295837\n",
      "1  1.098612  1.098612  0.693147  3.295837  0.000000  0.000000  0.000000\n",
      "2  2.197225  0.000000  0.000000  1.098612  0.000000  0.000000  0.000000\n",
      "3  0.000000  2.197225  0.000000  0.000000  0.000000  0.000000  1.098612\n",
      "4  0.000000  0.000000  0.000000  0.000000  1.098612  0.000000  0.000000\n",
      "5  0.000000  0.000000  2.079442  0.000000  0.000000  7.167038  0.000000\n"
     ]
    }
   ],
   "source": [
    "print(table.iloc[:, 7:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     london  research      cern   trinity  participated    guitar\n",
      "0  0.000000  0.000000  0.000000  0.000000      0.000000  0.000000\n",
      "1  0.000000  0.693147  0.000000  0.000000      0.000000  0.000000\n",
      "2  0.000000  1.386294  2.772589  0.000000      0.000000  0.000000\n",
      "3  0.000000  0.000000  0.693147  0.000000      3.583519  0.000000\n",
      "4  0.000000  0.000000  0.000000  0.000000      0.000000  0.000000\n",
      "5  7.167038  0.693147  1.386294  7.167038      0.000000  7.167038\n"
     ]
    }
   ],
   "source": [
    "print(table.iloc[:, 14:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       data  applications    worked      also  structures  auckland  computer\n",
      "0  0.546965      0.000000  0.000000  0.693147    2.079442  0.000000  0.000000\n",
      "1  0.182322      0.000000  1.386294  0.000000    0.000000  3.295837  0.000000\n",
      "2  0.000000      5.375278  2.079442  0.693147    0.000000  0.000000  0.000000\n",
      "3  0.182322      0.000000  2.079442  2.079442    0.000000  0.000000  0.000000\n",
      "4  0.364643      0.000000  0.000000  0.000000    2.079442  0.000000  5.375278\n",
      "5  0.182322      0.000000  0.000000  0.000000    0.693147  1.098612  0.000000\n"
     ]
    }
   ],
   "source": [
    "print(table.iloc[:, 20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__See full output in text format [here](https://github.com/anishLearnsToCode/bow-representation/blob/master/assets/tfidf.txt).__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
