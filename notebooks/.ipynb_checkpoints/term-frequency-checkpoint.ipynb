{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency (TF) Vector Representation\n",
    "__Anish Sachdeva (DTU/2K16/MC/013)__\n",
    "\n",
    "__Natural Language Processing (IT-425)__\n",
    "\n",
    "In this noteook we will extract Term Frequency vector representations from a given corpus, where our corpus will be my resume. We will divide the corpus into 6 different parts and each part will be treated as a document. The vector for a given word will be a $1 \\times 6$ vector and each column will represent the frequency countof how many times the word occured in that particular document. \n",
    "\n",
    "## 1. Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing all necesary packages\n",
    "from collections import Counter\n",
    "\n",
    "import pprint\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the Corpus (Resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anish sachdeva\n",
      "software developer + clean code enthusiast\n",
      "\n",
      "phone : 8287428181\n",
      "email : anish_@outlook.com\n",
      "home : sandesh vihar, pitampura, new delhi - 110034\n",
      "date of birth : 7th april 1998\n",
      "languages : english, hindi, french\n",
      "\n",
      "work experience\n",
      "what after college (4 months)\n",
      "delhi, india\n",
      "creating content to teach core java and python with data structures and algorithms and giving online classes to students.\n",
      "giving python classes workshops to students all around india and teaching core data structures and the python api\n",
      "with emphasis on data structures, algorithms and problem solving. see a sample python batch here:\n",
      "https://github.com/anishlearnstocode/python-workshop-6\n",
      "\n",
      "also teaching java to students in batches of 10 days, where the full java api and data types are covered along with many\n",
      "important algorithms are aso taught. see a sample java batch here: https://github.com/anishlearnstocode/java-wac-batch-32\n",
      "\n",
      "summer research fellow at university of auckland (2 months)\n",
      "auckland, new zealand\n",
      "worked on geometry of mobius transformations, differential geometry under dr. pedram hekmati at the department of\n",
      "mathematics, university of auckland. worked on various topics in mathematics such as abelian group theory,\n",
      "measure theory, graph theory and differential geometry.\n",
      "\n",
      "attended lectures and conferences with notable speakers from throughout academia and the industry. met up with ed witten\n",
      "who is currently at the forefront of applied mathematics in physics and mathematical topology in higher dimensional\n",
      "spaces.\n",
      "\n",
      "met with scientist from microsoft quantum research that are working on cutting edge research and using foundational\n",
      "group theory and mobius transformations in real world practical applications.\n",
      "\n",
      "software developer at cern (14 months)\n",
      "cern, geneva, switzerland\n",
      "worked in the core platforms team of the fap-bc group. part of an agile team of developers that maintains and adds core\n",
      "functionality to applications used internally at cern by hr, financial, administrative and other departments including\n",
      "scientific.\n",
      "\n",
      "worked on legacy applications that comprise of single and some times multiple frameworks such as java spring, boot,\n",
      "hibernate and java ee. also worked with google polymer 1.0 and jsp on the client side.\n",
      "\n",
      "maintained cern's electronic document handing system application with >1m loc that comprising of multiple frameworks\n",
      "and created ~20 years ago. worked on feature requests, support requests and incidents and also release cycles.\n",
      "\n",
      "while at cern, i also engaged socially and participated in self growth outside the work environment. i was part of the\n",
      "department band as lead singer and guitarist. i also worked on my french and learnt it till a2 level. i participated\n",
      "in many workshops, and volunteered as a participant and helper in many activities related to programming, robotics\n",
      "etc.\n",
      "\n",
      "teaching assistant (4 months)\n",
      "coding ninjas, delhi\n",
      "served as the teaching assistant to nucleus - java with ds batch, under mr. ankur kumar. worked on creating course\n",
      "content and quizzes for online platform of coding ninjas for java. helped students in core data structures and algorithms\n",
      "concepts in java.\n",
      "\n",
      "education\n",
      "delhi technological university (2016 - 2021)\n",
      "bachelors of technology mathematics and computing\n",
      "cgpa: 9.2\n",
      "\n",
      "the heritage school rohini (2004 - 2016)\n",
      "physics, chemistry, maths + computer science with english\n",
      "senior secondary: 94.8%\n",
      "secondary: 9.8 cgpa\n",
      "\n",
      "technical skills\n",
      "java + algorithms and data structures\n",
      "mean stack web development\n",
      "python + machine learning\n",
      "matlab + octave\n",
      "mysql, postgressql & mongodb\n",
      "\n",
      "other skills\n",
      "ms office, adobe photoshop, latex + mitex\n",
      "\n",
      "university courses\n",
      "applied mathematics i, ii, iii\n",
      "linear algebra + probability & statistics + stochastic processes + discrete maths\n",
      "computer organization & architecture + data structures + algorithm design and analysis + dbms + os\n",
      "computer vision + nlp\n",
      "\n",
      "important links\n",
      "https://www.linkedin.com/in/anishsachdeva1998/\n",
      "https://github.com/anishlearnstocode\n",
      "https://www.hackerrank.com/anishviewer\n",
      "\n",
      "honours and awards\n",
      "mitacs globalink scholarship cohort of 2020\n",
      "summer research fellowship university of auckland mathematics department\n",
      "technical student @ cern\n",
      "google india challenge scholarship\n",
      "\n",
      "certifications\n",
      "trinity college of london plectrum guitar grade 4 (distinction)\n",
      "trinity college of london plectrum guitar grade 3 (merit)\n",
      "trinity college of london plectrum guitar grade 2 (distinction)\n",
      "trinity college of london plectrum guitar grade 1 (distinction)\n",
      "french a2.1 level from cern\n",
      "java data structures and algorithms @ coding ninjas\n",
      "web development with ruby on rails @ coding ninjas\n",
      "competitive programming @ coding ninjas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_file = open('../assets/resume.txt', 'r')\n",
    "resume = resume_file.read().lower()\n",
    "resume_file.close()\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Resume\n",
    "we will now create a utility function called `tokenize` that will take our resume document and return us the tokens by removing stopwords, punctuations and numericals from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiity function to tokenize document\n",
    "def tokenize(document: str, stopwords_en=stopwords.words('english'), tokenizer=nltk.RegexpTokenizer(r'\\w+')):\n",
    "    document = document.lower()\n",
    "    return [token for token in tokenizer.tokenize(document) if token not in stopwords_en and token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anish', 'sachdeva', 'software', 'developer', 'clean', 'code', 'enthusiast', 'phone', 'email', 'outlook', 'com', 'home', 'sandesh', 'vihar', 'pitampura', 'new', 'delhi', 'date', 'birth', 'april', 'languages', 'english', 'hindi', 'french', 'work', 'experience', 'college', 'months', 'delhi', 'india']\n"
     ]
    }
   ],
   "source": [
    "# creating the tokens\n",
    "tokens = tokenize(resume)\n",
    "\n",
    "# printing first 30 tokens\n",
    "print(tokens[: 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dividing the Corpus Tokens into 6 Documents\n",
    "We now divide these tokens evenly into 6 different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third document contains the following tokens:\n",
      "['scientist',\n",
      " 'microsoft',\n",
      " 'quantum',\n",
      " 'research',\n",
      " 'working',\n",
      " 'cutting',\n",
      " 'edge',\n",
      " 'research',\n",
      " 'using',\n",
      " 'foundational',\n",
      " 'group',\n",
      " 'theory',\n",
      " 'mobius',\n",
      " 'transformations',\n",
      " 'real',\n",
      " 'world',\n",
      " 'practical',\n",
      " 'applications',\n",
      " 'software',\n",
      " 'developer',\n",
      " 'cern',\n",
      " 'months',\n",
      " 'cern',\n",
      " 'geneva',\n",
      " 'switzerland',\n",
      " 'worked',\n",
      " 'core',\n",
      " 'platforms',\n",
      " 'team',\n",
      " 'fap',\n",
      " 'bc',\n",
      " 'group',\n",
      " 'part',\n",
      " 'agile',\n",
      " 'team',\n",
      " 'developers',\n",
      " 'maintains',\n",
      " 'adds',\n",
      " 'core',\n",
      " 'functionality',\n",
      " 'applications',\n",
      " 'used',\n",
      " 'internally',\n",
      " 'cern',\n",
      " 'hr',\n",
      " 'financial',\n",
      " 'administrative',\n",
      " 'departments',\n",
      " 'including',\n",
      " 'scientific',\n",
      " 'worked',\n",
      " 'legacy',\n",
      " 'applications',\n",
      " 'comprise',\n",
      " 'single',\n",
      " 'times',\n",
      " 'multiple',\n",
      " 'frameworks',\n",
      " 'java',\n",
      " 'spring',\n",
      " 'boot',\n",
      " 'hibernate',\n",
      " 'java',\n",
      " 'ee',\n",
      " 'also',\n",
      " 'worked',\n",
      " 'google',\n",
      " 'polymer',\n",
      " 'jsp',\n",
      " 'client',\n",
      " 'side',\n",
      " 'maintained',\n",
      " 'cern',\n",
      " 'electronic',\n",
      " 'document',\n",
      " 'handing',\n",
      " 'system',\n",
      " 'application',\n",
      " 'loc']\n"
     ]
    }
   ],
   "source": [
    "k = len(tokens) // 6\n",
    "documents = []\n",
    "for i in range(5):\n",
    "    documents.append(tokens[i * k: (i + 1) * k])\n",
    "documents.append(tokens[5 * k:])\n",
    "\n",
    "print('The third document contains the following tokens:')\n",
    "pprint.pp(documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculating Most Common 5 Tokens from Each Document & Storing Frequency Tables for Each Document\n",
    "\n",
    "We take the 5 most common words from each document but they may not necessarily be unique and there might be some repition so we store them in set `most_common`. We also store the frequencies of words in all 6 documents in a `document_frequencies` list where each element is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = set()\n",
    "document_frequencies = []\n",
    "for document in documents:\n",
    "    frequencies = Counter(document)\n",
    "    document_frequencies.append(frequencies)\n",
    "    for word, frequency in frequencies.most_common(5):\n",
    "        most_common.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('structures', 3), ('computer', 3), ('algorithms', 2), ('java', 2), ('university', 2)]\n"
     ]
    }
   ],
   "source": [
    "# see the most common words in document 5\n",
    "print(document_frequencies[4].most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of most common words: 27\n"
     ]
    }
   ],
   "source": [
    "# number of most common words from all 6 documents (may not be 30)\n",
    "print('number of most common words:', len(most_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithms',\n",
      " 'also',\n",
      " 'applications',\n",
      " 'auckland',\n",
      " 'cern',\n",
      " 'college',\n",
      " 'com',\n",
      " 'computer',\n",
      " 'data',\n",
      " 'geometry',\n",
      " 'group',\n",
      " 'guitar',\n",
      " 'java',\n",
      " 'london',\n",
      " 'many',\n",
      " 'mathematics',\n",
      " 'participated',\n",
      " 'plectrum',\n",
      " 'python',\n",
      " 'requests',\n",
      " 'research',\n",
      " 'structures',\n",
      " 'students',\n",
      " 'theory',\n",
      " 'trinity',\n",
      " 'university',\n",
      " 'worked'}\n"
     ]
    }
   ],
   "source": [
    "# the most common words are\n",
    "pprint.pp(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculating Term Frequency (TF) Vectors\n",
    "We now calculate Term Frequency Vectors using the `document_frequencies` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'requests': [0, 0, 0, 2, 0, 0],\n",
      " 'structures': [3, 0, 0, 0, 3, 1],\n",
      " 'students': [3, 0, 0, 1, 0, 0],\n",
      " 'london': [0, 0, 0, 0, 0, 4],\n",
      " 'group': [0, 1, 2, 0, 0, 0],\n",
      " 'college': [1, 0, 0, 0, 0, 4],\n",
      " 'also': [1, 0, 1, 3, 0, 0],\n",
      " 'cern': [0, 0, 4, 1, 0, 2],\n",
      " 'mathematics': [0, 3, 0, 0, 2, 1],\n",
      " 'plectrum': [0, 0, 0, 0, 0, 4],\n",
      " 'computer': [0, 0, 0, 0, 3, 0],\n",
      " 'java': [2, 3, 2, 2, 2, 1],\n",
      " 'guitar': [0, 0, 0, 0, 0, 4],\n",
      " 'research': [0, 1, 2, 0, 0, 1],\n",
      " 'participated': [0, 0, 0, 2, 0, 0],\n",
      " 'data': [3, 1, 0, 1, 2, 1],\n",
      " 'algorithms': [2, 1, 0, 0, 2, 1],\n",
      " 'trinity': [0, 0, 0, 0, 0, 4],\n",
      " 'worked': [0, 2, 3, 3, 0, 0],\n",
      " 'geometry': [0, 3, 0, 0, 0, 0],\n",
      " 'com': [2, 1, 0, 0, 0, 3],\n",
      " 'auckland': [0, 3, 0, 0, 0, 1],\n",
      " 'university': [0, 2, 0, 0, 2, 1],\n",
      " 'many': [0, 1, 0, 2, 0, 0],\n",
      " 'applications': [0, 0, 3, 0, 0, 0],\n",
      " 'theory': [0, 3, 1, 0, 0, 0],\n",
      " 'python': [5, 0, 0, 0, 1, 0]}\n"
     ]
    }
   ],
   "source": [
    "vectors = {}\n",
    "for word in most_common:\n",
    "    vector = [0] * 6\n",
    "    for index, frequencies in enumerate(document_frequencies):\n",
    "        vector[index] = frequencies[word]\n",
    "    vectors[word] = vector\n",
    "pprint.pp(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 2, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# see the tf vector for any word (you can modify below)\n",
    "word = 'java'\n",
    "print(list(vectors[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Representing in Tabular Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   requests  structures  students  london  group  college  also  cern\n",
      "0         0           3         3       0      0        1     1     0\n",
      "1         0           0         0       0      1        0     0     0\n",
      "2         0           0         0       0      2        0     1     4\n",
      "3         2           0         1       0      0        0     3     1\n",
      "4         0           3         0       0      0        0     0     0\n",
      "5         0           1         0       4      0        4     0     2\n"
     ]
    }
   ],
   "source": [
    "table = pandas.DataFrame(data=vectors)\n",
    "print(table.iloc[:, 0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mathematics  plectrum  computer  java  guitar  research  participated\n",
      "0            0         0         0     2       0         0             0\n",
      "1            3         0         0     3       0         1             0\n",
      "2            0         0         0     2       0         2             0\n",
      "3            0         0         0     2       0         0             2\n",
      "4            2         0         3     2       0         0             0\n",
      "5            1         4         0     1       4         1             0\n"
     ]
    }
   ],
   "source": [
    "print(table.iloc[:, 8:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   data  algorithms  trinity  worked  geometry  com  auckland\n",
      "0     3           2        0       0         0    2         0\n",
      "1     1           1        0       2         3    1         3\n",
      "2     0           0        0       3         0    0         0\n",
      "3     1           0        0       3         0    0         0\n",
      "4     2           2        0       0         0    0         0\n",
      "5     1           1        4       0         0    3         1\n"
     ]
    }
   ],
   "source": [
    "print(table.iloc[:, 15:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   university  many  applications  theory  python\n",
      "0           0     0             0       0       5\n",
      "1           2     1             0       3       0\n",
      "2           0     0             3       1       0\n",
      "3           0     2             0       0       0\n",
      "4           2     0             0       0       1\n",
      "5           1     0             0       0       0\n"
     ]
    }
   ],
   "source": [
    "print(table.iloc[:, 22:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__See full output [here](https://github.com/anishLearnsToCode/bow-representation/blob/master/assets/tf.txt).__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
