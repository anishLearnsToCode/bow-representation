{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Vector Representation of Words\n",
    "__Anish Sachdeva (DTU/2K16/MC/13)__\n",
    "\n",
    "__Natural Language Processing - Dr. Seba Susan__\n",
    "\n",
    "In this notebook we se how to extract one hot vectors from a given corpus and in this case our corpus will be a resume. Our first step is importing all required packages.\n",
    "\n",
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the Corpus\n",
    "In this case our corpus will be the resume which we will divide into 6 parts and each part will represent a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anish sachdeva\n",
      "software developer + clean code enthusiast\n",
      "\n",
      "phone : 8287428181\n",
      "email : anish_@outlook.com\n",
      "home : sandesh vihar, pitampura, new delhi - 110034\n",
      "date of birth : 7th april 1998\n",
      "languages : english, hindi, french\n",
      "\n",
      "work experience\n",
      "what after college (4 months)\n",
      "delhi, india\n",
      "creating content to teach core java and python with data structures and algorithms and giving online classes to students.\n",
      "giving python classes workshops to students all around india and teaching core data structures and the python api\n",
      "with emphasis on data structures, algorithms and problem solving. see a sample python batch here:\n",
      "https://github.com/anishlearnstocode/python-workshop-6\n",
      "\n",
      "also teaching java to students in batches of 10 days, where the full java api and data types are covered along with many\n",
      "important algorithms are aso taught. see a sample java batch here: https://github.com/anishlearnstocode/java-wac-batch-32\n",
      "\n",
      "summer research fellow at university of auckland (2 months)\n",
      "auckland, new zealand\n",
      "worked on geometry of mobius transformations, differential geometry under dr. pedram hekmati at the department of\n",
      "mathematics, university of auckland. worked on various topics in mathematics such as abelian group theory,\n",
      "measure theory, graph theory and differential geometry.\n",
      "\n",
      "attended lectures and conferences with notable speakers from throughout academia and the industry. met up with ed witten\n",
      "who is currently at the forefront of applied mathematics in physics and mathematical topology in higher dimensional\n",
      "spaces.\n",
      "\n",
      "met with scientist from microsoft quantum research that are working on cutting edge research and using foundational\n",
      "group theory and mobius transformations in real world practical applications.\n",
      "\n",
      "software developer at cern (14 months)\n",
      "cern, geneva, switzerland\n",
      "worked in the core platforms team of the fap-bc group. part of an agile team of developers that maintains and adds core\n",
      "functionality to applications used internally at cern by hr, financial, administrative and other departments including\n",
      "scientific.\n",
      "\n",
      "worked on legacy applications that comprise of single and some times multiple frameworks such as java spring, boot,\n",
      "hibernate and java ee. also worked with google polymer 1.0 and jsp on the client side.\n",
      "\n",
      "maintained cern's electronic document handing system application with >1m loc that comprising of multiple frameworks\n",
      "and created ~20 years ago. worked on feature requests, support requests and incidents and also release cycles.\n",
      "\n",
      "while at cern, i also engaged socially and participated in self growth outside the work environment. i was part of the\n",
      "department band as lead singer and guitarist. i also worked on my french and learnt it till a2 level. i participated\n",
      "in many workshops, and volunteered as a participant and helper in many activities related to programming, robotics\n",
      "etc.\n",
      "\n",
      "teaching assistant (4 months)\n",
      "coding ninjas, delhi\n",
      "served as the teaching assistant to nucleus - java with ds batch, under mr. ankur kumar. worked on creating course\n",
      "content and quizzes for online platform of coding ninjas for java. helped students in core data structures and algorithms\n",
      "concepts in java.\n",
      "\n",
      "education\n",
      "delhi technological university (2016 - 2021)\n",
      "bachelors of technology mathematics and computing\n",
      "cgpa: 9.2\n",
      "\n",
      "the heritage school rohini (2004 - 2016)\n",
      "physics, chemistry, maths + computer science with english\n",
      "senior secondary: 94.8%\n",
      "secondary: 9.8 cgpa\n",
      "\n",
      "technical skills\n",
      "java + algorithms and data structures\n",
      "mean stack web development\n",
      "python + machine learning\n",
      "matlab + octave\n",
      "mysql, postgressql & mongodb\n",
      "\n",
      "other skills\n",
      "ms office, adobe photoshop, latex + mitex\n",
      "\n",
      "university courses\n",
      "applied mathematics i, ii, iii\n",
      "linear algebra + probability & statistics + stochastic processes + discrete maths\n",
      "computer organization & architecture + data structures + algorithm design and analysis + dbms + os\n",
      "computer vision + nlp\n",
      "\n",
      "important links\n",
      "https://www.linkedin.com/in/anishsachdeva1998/\n",
      "https://github.com/anishlearnstocode\n",
      "https://www.hackerrank.com/anishviewer\n",
      "\n",
      "honours and awards\n",
      "mitacs globalink scholarship cohort of 2020\n",
      "summer research fellowship university of auckland mathematics department\n",
      "technical student @ cern\n",
      "google india challenge scholarship\n",
      "\n",
      "certifications\n",
      "trinity college of london plectrum guitar grade 4 (distinction)\n",
      "trinity college of london plectrum guitar grade 3 (merit)\n",
      "trinity college of london plectrum guitar grade 2 (distinction)\n",
      "trinity college of london plectrum guitar grade 1 (distinction)\n",
      "french a2.1 level from cern\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_file = open('../assets/resume.txt', 'r')\n",
    "resume = resume_file.read().lower()\n",
    "resume_file.close()\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Resume\n",
    "We will now tokenize the Resume and obtain words/tokens from our corpus. We will further divide up these tokens into 6 parts to obtain our 6 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating utility function to tokenize the document\n",
    "def tokenize(document: str, stopwords_en=stopwords.words('english'), tokenizer=nltk.RegexpTokenizer(r'\\w+')) -> list:\n",
    "    document = document.lower()\n",
    "    return [token for token in tokenizer.tokenize(document) if token not in stopwords_en and token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anish', 'sachdeva', 'software', 'developer', 'clean', 'code', 'enthusiast', 'phone', 'email', 'outlook', 'com', 'home', 'sandesh', 'vihar', 'pitampura', 'new', 'delhi', 'date', 'birth', 'april', 'languages', 'english', 'hindi', 'french', 'work']\n"
     ]
    }
   ],
   "source": [
    "# getting the tokens\n",
    "tokens = tokenize(resume)\n",
    "# printing first 25 tokens\n",
    "print(tokens[: 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dividing Corpus into 6 documents\n",
    "We now divide the corpus into 6 documents, basically we equally divide the tokens list into 6 different fragments.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batches',\n",
      " 'days',\n",
      " 'full',\n",
      " 'java',\n",
      " 'api',\n",
      " 'data',\n",
      " 'types',\n",
      " 'covered',\n",
      " 'along',\n",
      " 'many',\n",
      " 'important',\n",
      " 'algorithms',\n",
      " 'aso',\n",
      " 'taught',\n",
      " 'see',\n",
      " 'sample',\n",
      " 'java',\n",
      " 'batch',\n",
      " 'https',\n",
      " 'github',\n",
      " 'com',\n",
      " 'anishlearnstocode',\n",
      " 'java',\n",
      " 'wac',\n",
      " 'batch',\n",
      " 'summer',\n",
      " 'research',\n",
      " 'fellow',\n",
      " 'university',\n",
      " 'auckland',\n",
      " 'months',\n",
      " 'auckland',\n",
      " 'new',\n",
      " 'zealand',\n",
      " 'worked',\n",
      " 'geometry',\n",
      " 'mobius',\n",
      " 'transformations',\n",
      " 'differential',\n",
      " 'geometry',\n",
      " 'dr',\n",
      " 'pedram',\n",
      " 'hekmati',\n",
      " 'department',\n",
      " 'mathematics',\n",
      " 'university',\n",
      " 'auckland',\n",
      " 'worked',\n",
      " 'various',\n",
      " 'topics',\n",
      " 'mathematics',\n",
      " 'abelian',\n",
      " 'group',\n",
      " 'theory',\n",
      " 'measure',\n",
      " 'theory',\n",
      " 'graph',\n",
      " 'theory',\n",
      " 'differential',\n",
      " 'geometry',\n",
      " 'attended',\n",
      " 'lectures',\n",
      " 'conferences',\n",
      " 'notable',\n",
      " 'speakers',\n",
      " 'throughout',\n",
      " 'academia',\n",
      " 'industry',\n",
      " 'met',\n",
      " 'ed',\n",
      " 'witten',\n",
      " 'currently',\n",
      " 'forefront',\n",
      " 'applied',\n",
      " 'mathematics',\n",
      " 'physics']\n"
     ]
    }
   ],
   "source": [
    "# dividing corpus into 6 documents\n",
    "k = len(tokens) // 6\n",
    "documents = []\n",
    "for i in range(5):\n",
    "    documents.append(tokens[i * k: (i + 1) * k])\n",
    "documents.append(tokens[4 * k:])\n",
    "\n",
    "# printing the second document\n",
    "pprint.pp(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculating 5 Most Common Tokens from Each Document\n",
    "we now calculate the 5 Most common Tokens from each document and it is not necessary taht we obtain $5 \\times 6 = 30$ tokens in the end as some tokens might be repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Most Common Tokens: 25\n",
      "{'algorithms',\n",
      " 'also',\n",
      " 'applications',\n",
      " 'auckland',\n",
      " 'cern',\n",
      " 'college',\n",
      " 'com',\n",
      " 'data',\n",
      " 'geometry',\n",
      " 'group',\n",
      " 'guitar',\n",
      " 'java',\n",
      " 'london',\n",
      " 'mathematics',\n",
      " 'participated',\n",
      " 'plectrum',\n",
      " 'python',\n",
      " 'requests',\n",
      " 'research',\n",
      " 'structures',\n",
      " 'students',\n",
      " 'theory',\n",
      " 'trinity',\n",
      " 'university',\n",
      " 'worked'}\n"
     ]
    }
   ],
   "source": [
    "# calculating most common 5 tokens from each document\n",
    "most_common = set()\n",
    "for document in documents:\n",
    "    frequencies = Counter(document)\n",
    "    for word, frequency in frequencies.most_common(5):\n",
    "        most_common.add(word)\n",
    "\n",
    "# print number of most common tokens\n",
    "print('Number of Most Common Tokens:', len(most_common), end='\\n')\n",
    "\n",
    "# print the most common tokens\n",
    "pprint.pprint(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating the One Hot Vectors for Each Word in the `most_common` Set\n",
    "We will iterate over all the words in `most_common` set and for each word we will se whether it is present in a particular document or not. If it is then we mark it as $1$ in the corresponding row otherwise as $0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "for word in most_common:\n",
    "    vector = np.zeros((6), dtype=int)\n",
    "    for index, document in enumerate(documents):\n",
    "        vector[index] = word in document\n",
    "    vectors[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "students: [[1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# let us see the vector output for a sample word, you can aso modify the word below to see one-hot vector representation\n",
    "word = 'students'\n",
    "print(word + ':', vectors[word].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: [[1 0 0 0 1 1]]\n",
      "also: [[1 0 1 1 0 0]]\n",
      "college: [[1 0 0 0 0 1]]\n",
      "mathematics: [[0 1 0 0 1 1]]\n",
      "applications: [[0 0 1 0 0 0]]\n",
      "structures: [[1 0 0 0 1 1]]\n",
      "research: [[0 1 1 0 0 1]]\n",
      "group: [[0 1 1 0 0 0]]\n",
      "java: [[1 1 1 1 1 1]]\n",
      "worked: [[0 1 1 1 0 0]]\n",
      "guitar: [[0 0 0 0 0 1]]\n",
      "students: [[1 0 0 0 1 1]]\n",
      "trinity: [[0 0 0 0 0 1]]\n",
      "theory: [[0 1 1 0 0 0]]\n",
      "plectrum: [[0 0 0 0 0 1]]\n",
      "auckland: [[0 1 0 0 0 1]]\n",
      "data: [[1 1 0 0 1 1]]\n",
      "london: [[0 0 0 0 0 1]]\n",
      "geometry: [[0 1 0 0 0 0]]\n",
      "participated: [[0 0 0 1 0 0]]\n",
      "com: [[1 1 0 0 0 1]]\n",
      "cern: [[0 0 1 1 0 1]]\n",
      "requests: [[0 0 0 1 0 0]]\n",
      "university: [[0 1 0 0 1 1]]\n",
      "algorithms: [[1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# we can see the vector representations for all the words in most_common set of words\n",
    "for word in most_common:\n",
    "    print(word + ':', vectors[word].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Making the Table of Words & Vector Representations\n",
    "We now create a $6 \\times 25$ table representing the words and their repective vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   python  also  college  mathematics  applications  structures  research  \\\n",
      "0       1     1        1            0             0           1         0   \n",
      "1       0     0        0            1             0           0         1   \n",
      "2       0     1        0            0             1           0         1   \n",
      "3       0     1        0            0             0           0         0   \n",
      "4       1     0        0            1             0           1         0   \n",
      "5       1     0        1            1             0           1         1   \n",
      "\n",
      "   group  java  worked  ...  auckland  data  london  geometry  participated  \\\n",
      "0      0     1       0  ...         0     1       0         0             0   \n",
      "1      1     1       1  ...         1     1       0         1             0   \n",
      "2      1     1       1  ...         0     0       0         0             0   \n",
      "3      0     1       1  ...         0     0       0         0             1   \n",
      "4      0     1       0  ...         0     1       0         0             0   \n",
      "5      0     1       0  ...         1     1       1         0             0   \n",
      "\n",
      "   com  cern  requests  university  algorithms  \n",
      "0    1     0         0           0           1  \n",
      "1    1     0         0           1           1  \n",
      "2    0     1         0           0           0  \n",
      "3    0     1         1           0           0  \n",
      "4    0     0         0           1           1  \n",
      "5    1     1         0           1           1  \n",
      "\n",
      "[6 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "table = pandas.DataFrame(data=vectors)\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
